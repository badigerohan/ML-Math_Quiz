{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f1f81f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a205e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "import tensorflow \n",
    "tensorflow.random.set_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b416c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8662358",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a9862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## x - images pixel\n",
    "## y - actual value\n",
    "\n",
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digits_logs/'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_WIDTH = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_HEIGHT*IMAGE_WIDTH*CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e935d85",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c450549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f6e4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a37b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8afa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee8a3f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5f9e91",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b8e192c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "038ca38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afab14e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9adb8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baf7e619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c482066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "790d198e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c2236",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "025e84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rescale (easy for neural network to process)\n",
    "x_train_all, x_test = x_train_all/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7f80b",
   "metadata": {},
   "source": [
    "#### Convert target values to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40236e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1641d67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = y_train_all[:5]\n",
    "np.eye(10)[values]\n",
    "## no of classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7173231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e650da3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4775f886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2deec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c07fed8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088abe58",
   "metadata": {},
   "source": [
    "### Create validation dataset from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e127a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50ae24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d35e08df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d5727d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d400d",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce403dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e932c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statring setup for tensor, also used for graph\n",
    "X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X') ## features\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels') ## labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4cec46",
   "metadata": {},
   "source": [
    "## Neural Network Archi\n",
    "\n",
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb0bbca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nPara which we are gonna determine ahead of time. These include how long to \\ntrain the model, no of epoch,learning rate in optimiser, no of layers \\nin neural network\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Para which we are gonna determine ahead of time. These include how long to \n",
    "train the model, no of epoch,learning rate in optimiser, no of layers \n",
    "in neural network\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f4cfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77c22e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for layer\n",
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    with tf.name_scope(name):\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim,stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value=initial_w, name='W') \n",
    "\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value=initial_b, name='B')\n",
    "\n",
    "        layer_in = tf.matmul(input, w) + b\n",
    "        \n",
    "        if name=='out':\n",
    "            layer_out = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.nn.relu(layer_in)\n",
    "            \n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "            \n",
    "        return layer_out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0489df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Model without dropout\n",
    "# layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1],\n",
    "#                      bias_dim=[n_hidden1], name='layer_1') \n",
    "\n",
    "# layer_2 = setup_layer(layer_1, weight_dim=[n_hidden1, n_hidden2],\n",
    "#                      bias_dim=[n_hidden2], name='layer_2') \n",
    "\n",
    "# output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES],\n",
    "#                      bias_dim=[NR_CLASSES], name='out') \n",
    "\n",
    "# model_name = f'{n_hidden1}-{n_hidden2} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8737843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1],\n",
    "                     bias_dim=[n_hidden1], name='layer_1') \n",
    "\n",
    "layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "\n",
    "layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2],\n",
    "                     bias_dim=[n_hidden2], name='layer_2') \n",
    "\n",
    "output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES],\n",
    "                     bias_dim=[NR_CLASSES], name='out') \n",
    "\n",
    "model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'\n",
    "## DO - dropout\n",
    "## LR - learning rate\n",
    "## E - epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4992c6b",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c5dc7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Created!\n"
     ]
    }
   ],
   "source": [
    "## Folder for Tensorboard\n",
    "folder_name = f'{model_name} at {strftime(\"%H %M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Directory Created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c33a67",
   "metadata": {},
   "source": [
    "# Loss, Optimisation & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ea1f9",
   "metadata": {},
   "source": [
    "#### Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb2e0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_calc'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y,\n",
    "                     logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754f6af",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "533957dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    ## minimize the loss when we iterate over data\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819544ee",
   "metadata": {},
   "source": [
    "#### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df562ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    model_prediction = tf.argmax(output, axis=1, name='prediction')\n",
    "    correct_pred = tf.equal(model_prediction, tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e67111ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating summary\n",
    "with tf.name_scope('performance'):\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost_or_loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259259a",
   "metadata": {},
   "source": [
    "#### Check input images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bb4eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## showing how images are in trained model \n",
    "with tf.name_scope('show_image'):\n",
    "    x_image = tf.reshape(X, [-1,28,28,1]) ## (no of images,hei,wid,ch)\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56d8ef",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d27601d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "## tensorflow session is when placeholder start getting its value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e07b3",
   "metadata": {},
   "source": [
    "#### Setup Filewriter & Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0c857ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## summary = graphs\n",
    "merged_summary = tf.summary.merge_all()## merge all graphs\n",
    "\n",
    "train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory + '/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a3b91",
   "metadata": {},
   "source": [
    "#### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96f45e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff204e9",
   "metadata": {},
   "source": [
    "### Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79b2ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11564fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples/size_of_batch)\n",
    "\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b8a5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > num_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    \n",
    "    return data[start:end], labels[start:end]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a6d4b",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a76f3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0 \t| Training Accuracy = 0.8510000109672546\n",
      "Epoch1 \t| Training Accuracy = 0.8640000224113464\n",
      "Epoch2 \t| Training Accuracy = 0.8709999918937683\n",
      "Epoch3 \t| Training Accuracy = 0.8700000047683716\n",
      "Epoch4 \t| Training Accuracy = 0.871999979019165\n",
      "Epoch5 \t| Training Accuracy = 0.8759999871253967\n",
      "Epoch6 \t| Training Accuracy = 0.9769999980926514\n",
      "Epoch7 \t| Training Accuracy = 0.9769999980926514\n",
      "Epoch8 \t| Training Accuracy = 0.984000027179718\n",
      "Epoch9 \t| Training Accuracy = 0.9819999933242798\n",
      "Epoch10 \t| Training Accuracy = 0.9819999933242798\n",
      "Epoch11 \t| Training Accuracy = 0.984000027179718\n",
      "Epoch12 \t| Training Accuracy = 0.9850000143051147\n",
      "Epoch13 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch14 \t| Training Accuracy = 0.984000027179718\n",
      "Epoch15 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch16 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch17 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch18 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch19 \t| Training Accuracy = 0.9860000014305115\n",
      "Epoch20 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch21 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch22 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch23 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch24 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch25 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch26 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch27 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch28 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch29 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch30 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch31 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch32 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch33 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch34 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch35 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch36 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch37 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch38 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch39 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch40 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch41 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch42 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch43 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch44 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch45 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch46 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch47 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch48 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch49 \t| Training Accuracy = 0.9940000176429749\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nr_epochs):\n",
    "    \n",
    "    ## -----Training Dataset-------\n",
    "    for i in range(nr_iterations):\n",
    "        ## returns data & label\n",
    "        batch_x, batch_y = next_batch(batch_size=size_of_batch, data=x_train\n",
    "                                      ,labels=y_train)\n",
    "        ## setting the data\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        ## running the calculations\n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "    ## minizise loss(train_step) on feed_dictionary & will update the weights\n",
    "        \n",
    "    ## Summary at every epoch\n",
    "    s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], \n",
    "                                  feed_dict=feed_dictionary)\n",
    "    ## take the summary(s) and write it on disk(epoch)\n",
    "    train_writer.add_summary(s, epoch)\n",
    "    \n",
    "    print(f'Epoch{epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "    ## -----Validation Dataset-------\n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
    "    validation_writer.add_summary(summary, epoch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea1db",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "825683dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-48-4d76ed178320>:3: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:200: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: SavedModel\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "outputs = {'accuracy_calc/prediction': model_prediction}\n",
    "inputs = {'X':X}\n",
    "tf.compat.v1.saved_model.simple_save(session=sess, export_dir='SavedModel', \n",
    "                                 inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0c74a",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e11ac43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABAElEQVR4nO2Vyw2DMAyG3ap3RoAVmMCBrXICbwIbMEEEmzBCmMA9VFR9EGLTqhIV3xET/vjHjxMzM/yQ8y/FDsEgRAREtEnwJC2aWWAYhqfnzjmVoChDIroLISI45+5C2kwvsRemaYK+76HrOkiS5CmGiG8ZR2EB4zgGY0VRcF3Xks8wM7PI0jRNgzFEVCW4j7b4CLH5C3jv2RjD3nvxmWiVhphbJc/zt+pdY5NgWZYAcCuYqqp0hzUWWmvZGMPWWrX9KksfJ03TNKttEmNV8HWkqe1bIDi8iQjatoUsy8TNLbnQ6rbQDObZiej22Pz3F5DMVPE+/Bb/P0sPwf0LXgGAJwNqzP5nHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28 at 0x1F8312A6C10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('MNIST/test_img.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "979a8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to grayscale \n",
    "bw = img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09254d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Invert image (as model is trained with black bg & white text)\n",
    "img_array = np.invert(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9c42813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2802a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flattening img, i.e- (28,28) to (784)\n",
    "test_img = img_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0114805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b08b4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Return highest prob for the image\n",
    "prediction = sess.run(fetches=tf.argmax(output, axis=1), \n",
    "                      feed_dict={X:[test_img]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e936ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for text img (ans=2) = [2]\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for text img (ans=2) = {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4da0ed",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1dc1bb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset = 97.82%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = sess.run(fetches=accuracy, \n",
    "                         feed_dict={X:x_test, Y:y_test})\n",
    "print(f'Accuracy on test dataset = {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36ac65",
   "metadata": {},
   "source": [
    "# Reset for next run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e54ae497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_writer.close()\n",
    "# validation_writer.close()\n",
    "# sess.close()\n",
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2c1c1",
   "metadata": {},
   "source": [
    "# Code used in 1st part of module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48079494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FOR 1st HIDDEN LAYER\n",
    "# with tf.name_scope('hidden_1'):\n",
    "#     ## staring values to weights & bias\n",
    "#     initial_w1 = tf.truncated_normal(shape=[TOTAL_INPUTS,n_hidden1],\n",
    "#                                     stddev=0.1, seed=42)\n",
    "#     ## truncated means there will not be any extreme values on tail left or right\n",
    "#     ## shape(no of inputs, no of neurons)\n",
    "\n",
    "#     ## Variable willl hold weights in the 1st hidden layer\n",
    "#     w1 = tf.Variable(initial_value=initial_w1, name='w1') \n",
    "    \n",
    "#     initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\n",
    "#     b1 = tf.Variable(initial_value=initial_b1, name='b1')\n",
    "\n",
    "#     ## inputs in for the 1st layer\n",
    "#     layer1_in = tf.matmul(X, w1) + b1\n",
    "    \n",
    "#     layer1_out = tf.nn.relu(layer1_in)\n",
    "#     ## nn.relu is the activation fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a93848c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# All is setup is for 1st hidden layer, the weights & biases will be upadted\n",
    "# during the training process. These values will be fed into in the activation\n",
    "# function of neurons and represent the strength of conections b/w diff units\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52bc0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FOR 2nd HIDDEN LAYER\n",
    "# with tf.name_scope('hidden_2'):\n",
    "#     initial_w2 = tf.truncated_normal(shape=[n_hidden1,n_hidden2],\n",
    "#                                     stddev=0.1, seed=42)\n",
    "#     w2 = tf.Variable(initial_value=initial_w2, name='w2') \n",
    "\n",
    "#     initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\n",
    "#     b2 = tf.Variable(initial_value=initial_b2, name='b2')\n",
    "\n",
    "#     layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "#     layer2_out = tf.nn.relu(layer2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66548dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FOR 3rd (OUTPUT) LAYER\n",
    "# with tf.name_scope('output_layer'):\n",
    "\n",
    "#     initial_w3 = tf.truncated_normal(shape=[n_hidden2, NR_CLASSES],\n",
    "#                                     stddev=0.1, seed=42)\n",
    "#     w3 = tf.Variable(initial_value=initial_w3, name='w3') \n",
    "\n",
    "#     initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\n",
    "#     b3 = tf.Variable(initial_value=initial_b3, name='b3')\n",
    "\n",
    "#     layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "#     output = tf.nn.softmax(layer3_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9dbc5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b3.eval(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff155cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d664b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b60da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b04af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe720a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04421c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efd462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d86080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
